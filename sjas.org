* sjas
:PROPERTIES:
:ID:       53f41b8a-07ec-42d1-a40b-15900dc903cf
:END:

#+BEGIN_20220630Z
Dear Boris,

Part I:

> this correspondence should be understood as a starting point for a new considerations about what logic and computation are truly about

I agree that the ultimate goal is a deeper understanding of formal
systems, for which the Curry-Howard Correspondence per se is an
instrument rather than an end. 

In particular, I am interested in the _autarkic_ aspects of formal
systems, that is, the expressivity available to a given system, and
the degree to which that expressivity is dependent on systems outside
of the one under consideration. By a certain pigeonhole principle
intuition, if one wishes to say something about a system, a
meta-system is required. The infinite regress of meta-systems is
closed in certain respects when a system becomes expressive (strong)
enough to represent itself. However, although self-representation
allows a system to see itself, it does not follow that the system can
now speak of itself [0], or at least that question of self-speaking,
and the knowledge that such a behavior would entail, becomes a new
topic of investigation.

The effects of self-seeing and self-speaking on self-knowledge are
brought into sharp relief by the limitative theorems of mathematical
logic, computation, etc., and the same generativity applies: now that
there is a boundary governing the limits of self-knowledge, the shape
of that boundary is of interest.

The works of Girard, Seiller, and yourself on this boundary [1], and
the broader question of autarky in formal systems, is my primary
motivating interest in Stellar Resolution, and thus the question of
application of the Curry-Howard Correspondence to logic programming. I
believe that an executable variant of SR, i.e. a programming language
based upon the SR deductive system, in a similar fashion to the basis
of Prolog upon the Selective Linear Definite clause resolution rule,
would be useful not only to further the study of SR itself, but also
to see what consequences the properties of SR have for "live" formal
systems - which is to say, software [4].

Part II:

> logic programming is a sort of logical hijacking allowing to do programming with logical entities, hence it is both computational and logical in some sense (but not in the sense of Curry-Howard).

I am sympathetic to the difference between logic programming and
"standard" functional programming, and would be curious as to further
commentary on this divergence from the usual sense of Curry-Howard. It
appears to me that the core reason should be due to the properties of
the logical proof systems to which the different computational systems
correspond, e.g. Natural Deduction and the Simply Typed Lambda
Calculus, versus Resolution and <whatever computational system with
which resolution corresponds>. The attempt to build this functor may
be in vain, however, precisely because, as you say, logic programming
"is both computational and logical" - the correspondence appears to
not be universally a statement of the mapping between two domains, but
may need to be extended with an automorphism as well.

In other respects, though, the functor appears viable: Consider that
the Lambda Calculus does not suffice for computation per se, but can
be equipped with an evaluation strategy (ES), a choice of how to
perform reductions (and thus more precisely, a Reduction Strategy,
RS), giving it the dynamism necessary to actually manipulate
symbols. This evaluation strategy can be encoded in the LC+RS, via a
self-interpreter. As discussed in Part I, this self-interpretation in
one respect closes the loop on the infinite regress of seeing and
speaking about LC+ES, but opens other questions about knowledge, by
the importation of logical antinomies into the computational domain
(paradigmatically, by introducing Cantorian Diagonalization paradoxes
via the Halting Problem [5]).

Analogously, the computational interpretation of SLD requires an
evaluation strategy, a choice of how to perform goal selection (GS),
and how to search the refutation tree (Tree Search, TS). Similarly, a
self-interpreter for SLD+GS+TS can be written, _as evidenced by_
Prolog self-interpreters. Note this experimental, experiential factor:
the writing of a self-interpreter that executes, that lives and
breathes separate from the mind of the logician, or its paper
specification, is the great differentiator of the study of computation
from mathematics more generically: the subject of the computationalist
has physical substance, it is incarnated philosophy.

I would thus be very curious as to how Stellar Resolution relates to
the above framework for closing the loop on executability, and
exploring its capacity for formal autarky, via an executable artifact
instantiating the properties of the SR system.

Part III: 

> What would cut-elimination look like? Are proofs sort of a certificate asserting how to get to an error from a set of atomic statements (contradiction in the specification system)? Is such a system related at all to the computational meaning of classical logic?

I believe the work of Will Byrd [6] on purely relational logic
programming in miniKanren [7] is relevant to the question of
cut-elimination. miniKanren has been used to implement purely
relational (no cut rule used) interpreters for Turing Complete
languages, thus demonstrating the computational universality of pure
relational resolution as a computational system. The restrictions of
pure relational programming also bring clarity to problems of negation
[8] that seem highly relevant to matters of constructivity in logic,
and to Girard's work on negation-as-swapping.

Thank you very much for answering my question, and I hope that this
provides sufficient background for my interest in your work.

Sincerely,
James Torre
--jpt4

[0] "Whereof one cannot speak, thereof one must be silent."

[1] Among others, e.g. Dan Willard's Self-Justifying Axiom Systems, and Brown and Palsberg's "Self-Interpreter" (a sleight of hand, I think, but illuminating nonetheless) for F-Omega. See these posts [2] [3] for elaboration on the interrelation of these two.

[2] https://cstheory.stackexchange.com/questions/51579/is-there-a-relationship-between-brown-and-palsbergs-self-interpreter-for-f-omeg

[3] https://cstheory.stackexchange.com/questions/51580/is-there-a-relation-between-the-techniques-used-by-dan-willard-versus-those-of

[4] And in particular, self-modifying artificial agents.

[5] And thus the relevance of Lawvere's categorification of Cantor: https://arxiv.org/abs/math/0305282

[6] http://webyrd.net/

[7] http://minikanren.org/

[8] https://scholarworks.iu.edu/dspace/bitstream/handle/2022/8777/Byrd_indiana_0093A_10344.pdf 
From the Conclusion, p. 238-240

> After trying to wrangle a few recalcitrant relations into termination, we may be tempted to abandon the
relational paradigm, and use miniKanren’s impure features like conda and project.
We might then view miniKanren as merely a “cleaner”, lexically scoped version of
Prolog, with S-expression syntax and higher-order functions. However tempting
this may be, we lose more than the flexibility of programs once we abandon the
relational approach: we lose the need to construct creative solutions to difficult yet
easily describable problems, such as the rembero problem in Chapter 7.

> Haskell programmers have learned, and are still learning, to avoid explicit
effects by using an ever-expanding collection of monads; miniKanren programmers
are learning to avoid divergence by using an ever-expanding collection of declarative
techniques, many of which express limited forms of negation in a bottom-avoiding
manner. Haskell and miniKanren show that, sometimes, painting yourself into a
corner can be liberating.
.
A final, very speculative observation: it may be possible to push the analogy
between monads and techniques for bottom avoidance further. Before Moggi’s work
on monads (Moggi 1991), the relationship between different types of effects was not
understood—signaling an error, printing a message, and changing a variable’s value
in memory seemed like very different operations. Moggi showed how these apparently
unrelated effects could be encapsulated using monads, providing a common
framework for a wide variety of effects. Could it be that the various types of divergence
described in Chapter 5 are also related, in a deep and fundamental way?

> Indeed, divergence itself is an effect. From the monadic viewpoint, divergence is
equivalent to an error, while from the relational programming viewpoint, divergence
is equivalent to failure; is there a deeper connection?

Resolution systems without cut rule = purely relational logic programming.

Fully relational logic programming still requires a choice of ES, but one which excludes the cut rule (or so I speculatively characterize it). Will Byrd's work on miniKanren demonstrates that a fully relational LC interpreter is possible, and thus fully relational self-interpretation of resolution+ES is possible.

Taken from this perspective

 I believe that the properties of a formal system benefit from realizing them in a way that allows for experimental interaction. For example, in computability, not only that which implements Turing Machines directly, but anything that can, through a long chain of hidden computation, emulate Turing Machines, allows for arbitrary code execution; this is the motivation behind the LangSec programme, to implement automatic restrictions in expressivity at the level of language, because it is difficult to verify and restrict the interactions of the permitted statements within a language. In the other direction,  I just spent a very long time attempting to implement a certain kind of fully relational negation operation in miniKanren, which demonstrated how certain aspects of the relations manifest themselves despite my best efforts to bypass/flank/work around them.

There are other kinds of autarky than self-interpretation: secure self-knowledge - SJAS. I am curious whether a system based on Stellar Resolution can be equipped with similar gnothi seauton.
#+END_20220630Z

:20220614Z:

We do not directly employ the features of a Turning Machine that
distinguish its computational expressivity from DFAs and push-down
automata, namely, the infinite tape. Instead, it is a side-effect of
the particular structure of its memory that programming with respect
to the TM model is more ergonomic than with DFAs and PDAs. 

1) In the reverse, while limitative theorems are rarely encountered,
   systems which are guaranteed to be free of the restrictions imposed
   by a limit may in turn offer ergonomic benefits.

2) The dual of never using the full power of a sufficiently expressive
   model of computation is that the limitative theorems are
   ever-present, and so evading their restrictions, despite at the
   cost of the more powerful MoC, is net beneficial.

:END:

20220611Z

https://twitter.com/arsmachinae/status/1535748295989071874

> i've neglected it for a long time under the impression that all
people generally think the same way
but i do think that underselling one's cognitive capabilities through
not knowing their own mind's language is an issue of our time

Gnothi seauton. PRC will know its own mind.

https://cstheory.stackexchange.com/questions/51581/what-is-the-model-of-computation-that-corresponds-in-the-manner-of-curry-howard

What is the model of computation that corresponds (in the manner of
Curry-Howard) to the deduction rule of resolution?

The Curry-Howard Correspondence is well-documented for the isomorphism
which associates the intuitionistic natural deduction proof calculus
(logic side) with the type system for the simply typed lambda calculus
(computation side). To produce a proof, the proof calculus must be
equipped with a proof procedure [0]; to produce a normalized type, the
type system must be equipped with a type inference algorithm. What is
the model of computation which corresponds to the rule of logical
resolution [1], as is used in, e.g., Prolog? What is the computation
side equivalent of the proof search procedure used to produce the
resolvents of a resolution-based proof calculus?

I inquire because, from an intuitive perspective, logic programming
languages such as Prolog appear to directly instantiate the logic side
of the Correspondence. Types can be satisfied by multiple terms, and
thus multiple programs can prove the same logical
proposition. However, logic programming terms are exactly logical
propositions, and programs directly manipulate such propositions. Is
it the case that Correspondence is one-sided for resolution-based
systems? Is the Correspondence for resolution "recurrent" (i.e. the
model of computation and logic are the same entity)?

[0] https://en.wikipedia.org/wiki/Proof_procedure

[1] https://en.wikipedia.org/wiki/Resolution_(logic)

Tags
curry-howard
lo.logic
first-order-logic
proof-theory
typed-lambda-calculus

https://cstheory.stackexchange.com/questions/51579/is-there-a-relationship-between-brown-and-palsbergs-self-interpreter-for-f-omeg

Is there a relationship between Brown and Palsberg's Self-Interpreter
for F-Omega and Lawvere's Fixed Point Theorem?

Brown and Palsberg [0] demonstrated an self-interpreter for
F-Omega. To do so, they perform "a careful analysis of the classical
theorem [of the impossibility of self-interpretation by total
languages]", to "show that static type checking in Fω can exclude the
proof’s diagonalization gadget, leaving open the possibility for a
self-interpreter".

There has been some debate [1] over the precise definition of
"interpreter" and "self-interpreter" within [0], and more broadly in
the field. The general impression I draw from this discussion is that
the "representation" and "reduction" of a program are a spectrum, with
different evaluators performing different degrees of
representation/reduction, partially dependent on the design of the
evaluator, as bounded by the capabilities of the implementation
language.

However, my question does not concern whether Brown and Palsberg's
self-interpreter accomplishes interpretation, but instead whether and
how their mechanism of forbidding diagonalization relates to other
domains. As analogy, I appeal to Lawvere's Fixed Point Theorem (LFPT),
which has been used to unify the presentation of a number of
limitative theorems proven using diagonalization methods [2].

Can Brown and Palsberg's evasion of the effects of diagonalization be
applied with similar breadth? Can their diagonalization-exclusion
mechanism be translated to other diagonalization-based proofs? How
does it stand with respect to the preconditions for the Fixed Point
Theorem (e.g., does the LPFT simply not apply to the problem of total
language self-interpretation)?

A further extension of this question would be to compare Brown and
Palsberg's mechanism with Dan Willard's mechanism for avoiding
diagonalization in his Self-Justifying Axiom Systems.

[0] http://compilers.cs.ucla.edu/popl16/

[1] http://math.andrej.com/2016/01/04/a-brown-palsberg-self-interpreter-for-godels-system-t/

[2] https://arxiv.org/abs/math/0305282

Tags: fixed-points proof-theory pl.programming-languages

https://cstheory.stackexchange.com/questions/51580/is-there-a-relation-between-the-techniques-used-by-dan-willard-versus-those-of

Is there a relation between the techniques used by Dan Willard, versus
those of Brown and Palsberg, to exclude diagonalization?

This question extends my inquiry from a previous post [0].

Dan Willard's Self-Justifying Axiom Systems/Self-Verifying Theories
[1] and Brown and Palsberg's self-interpreter for F-Omega [2] both
employ techniques that might be described as "excluding
diagonalizaton" from a given domain, while preserving other properties
usually considered as concomitant with diagonalization.

Willard: "In outline, the key to Willard's construction of his system
is to formalise enough of the Gödel machinery to talk about
provability internally without being able to formalise
diagonalisation. Diagonalisation depends upon being able to prove that
multiplication is a total function (and in the earlier versions of the
result, addition also). Addition and multiplication are not function
symbols of Willard's language; instead, subtraction and division are,
with the addition and multiplication predicates being defined in terms
of these. Here, one cannot prove the PI-0-2 sentence expressing
totality of multiplication:"

Brown and Palsberg: "After a careful analysis of the classical
theorem, we show that static type checking in Fω can exclude the
proof’s diagonalization gadget, leaving open the possibility for a
self-interpreter."

Therefore: Are these techniques related?

Separate from this core question, but possibly useful for answerers, I
speculate that, if so, the relation might be characterizable via a
common bypassing of Lawvere's Fixed Point Theorem, which itself
unifies a number of diagonalization based proofs.

[0] Is there a relationship between Brown and Palsberg's
Self-Interpreter for F-Omega and Lawvere's Fixed Point Theorem?

[1] https://en.wikipedia.org/wiki/Self-verifying_theories

[2] http://compilers.cs.ucla.edu/popl16/

Tags:
fixed-points
proof-theory
proof-complexity
pl.programming-languages
ct.category-theory

NY Category Theory Seminar

https://twitter.com/ilaba/status/1535646826077904896

I am interested in charting the boundary of limitative theorems, and
at present am "charting the charts", cataloguing the prior work of
others who have drawn lines on the map between the securely known and
the wilder regions.

http://compilers.cs.ucla.edu/popl16/popl16-full.pdf

> At first, a classical theorem in computability theory seems to imply
that a self-interpreter for Fω is impossible. Fortunately, further
analysis reveals that the proof relies on an assumption that a
diagonalization gadget can always be defined for a language with a
self-interpreter. We show this assumption to be false: by using a
typed representation, it is possible to define a self-interpreter such
that the diagonalization gadget cannot possibly type check

Rigorization of interfaces between connectionist systems/agents is
important because consciousness is locused at the interface/flux of
communicating systems. SJAS cannot be tricked into corrupting its
knowledge base with inconsistency, or into thinking its knowledge base
is inconsistent.

Concentric/consecutive attending systems can approach the recurrence
of consciousness, but require at least three stages:

object <- attender <- meta-attender

Meta-attender can thus have a reference for attending, but only
strictly downwards. Meta-attenders of increasing degree have access to
data modelling increasingly complicated unilateral cognitive
architectures. The concentric/consective structure could be a lattice
or DAG rather than a poset; a recurrent, properly self-attending
conscious system could contain a loop of arbitrary length.

object <- attender-1 <------------------- meta-attender-2
   |-<--- attender-2 <- meta-attender-1 <-----|

meta-attender-2 has more complex sample cognition data than
meta-attender-1.

object <- attender <- meta-attender-1
             |             ^
             V             |
          meta-attender-2 -|

attender is the locus of consciousness, attending to ma2 attending to
ma1 attending to attender, thus closing the recurrence.

Q: If attending can be multiplexed, why not have a single layer
self-attender?

A: Because attending consumes physical area as a resource, like
silicon on an IC. A "self-attender" is not two entities in spatial
superposition (though the loop may be formed by fast switching between
multiple functional configurations of the same subtrate, a kind of
temporal superposition). Thus, a s-a can be represented as two
distinct sub-systems.

Temporal superposition:

attender A is in configuration C1 at times t = 1 mod 2, and
configuration C2 at t = 0 mod 2. Each configuration is spare, insofar
as there are non-active elements in each configuration. These unused
elements are at least partially complementary, in that some are used
in C1 and not used C2, and others vice-versa. This allows the
complementary unused elements to act as echoic memory of the state of
A in configuration C(t), for A in C(t+1). WLOG, C1 can attend to C2,
and vice-versa, forming the recurrence. Because the configurations are
sparse and not complementary (thus not disjoint), A is not fully
partitionable into A1 and A2 sub-systems.

20220610Z 

A category theoretic proof of Goedel's 2nd Incompleteness Theorem,
using Joyal's Arithmetic Universes:
https://twitter.com/jpt401/status/1535077983945232384

:20220609Z:
[[id:029dd4e4-bbd1-4a9e-bae6-e1498c93af6a][curry-howard-resolution]]
Curry-Howard Correspondence for resolution based proof systems.

https://www.lix.polytechnique.fr/~lutz/papers/tableau-def.pdf

> This certainly makes perfect sense from the viewpoint of functional
programming and the Curry-Howardcorrespondence, where proofs are
programs and the proof normalization is the execution of the
program. However, from the viewpoint of logic programming and proof
search, this only makes little sense, since all considered proofs are
already in normal form.

How expressive can an SJAS be when employing a combinatorial proof
based deduction system?
:END:

:20220608Z:
#+BEGIN_PROGRAMME
A Preliminary Research Programme for Autarkic [0] Logic

Can Dan Willard's work on Goedel's Second Incompleteness Theorem be
extended to other limitative theorems in mathematical logic and
programming language theory? Willard's Self-Justifying Axiom Systems
[1] provide a framework for precisely characterizing the features of a
formal system required for Goedel's Second Incompleteness Theorem to
apply. In particular, SJAS are formal systems which retain 1)
consistency relative to Peano Arithmetic and 2) _self-provability of
consistency_, at the cost of weakened, carefully tuned system
expressivity [2]. Any systems stronger than SJAS thus satisfy the
prerequisites for the 2nd Incompleteness Theorem to take effect,
precluding a formal system from simultaneously i) being consistent and
ii) containing a proof of its own consistency.

From Lawvere's Fixed Point Theorem (LFPT) [4], we know that many
limitative results have a common structure. Lawvere, using
category theoretic formulations, and Yanovsky [5], using set theoretic
equivalents, have demonstrated how to use LFPT to prove
1. Cantor’s theorem that N < P(N) (the infinity of the Naturals is
   strictly less than that of the Reals)
2. The inadmissibility of Russell’s paradox
3. The non-definability of satisfiability
4. Tarski’s non-definability of truth and
5. Godel’s first incompleteness theorem.
6. Turing's undecidability of the Halting Problem

among other limitative theorems in other areas of mathematics. This
commonality can be extended, in particular due to the prevalence of
mathematical objects that can encode algorithmic behavior, and thus be
treated as instances of the Halting Problem.

Additionally, it is known that Turing's result can be used to prove
Goedel's [6]. From Undecidability, thus Incompleteness: U -> I - does
~I -> ~U also hold? The absence of Goedel's Second Incompleteness
Theorem from the above presents a gap that, if filled, could connect
Willard's SJAS with Lawvere's FPT: if the 2nd Inc Thm could be
characterized in terms of the FPT, then the features of SJAS which
allow them to evade the Incompleteness Effect could be analyzed, to
determine whether the features of such systems are _similarly
generalizable_ to those fields whose limitative results the FPT
subsumes [7].

Willard touches on the epistemological implications of his work for
"Thinking Beings" in [9]. Formal reasoning conducted within SJAS
benefits from strong epistemic security: positive answers can be given
to the question of "Is this reasoning system consistent?", without
risk of encountering future deductions which contradict this
conclusion, and without reliance on an external meta-system. An
artificial agent _defined as_ an SJAS would therefore be the maximally
expressive agent that retains autarkic confidence in its ability to
make arbitrary statements about its body of knowledge, including
self-knowledge.

[0] "Self-powered"; a logic able to provide strong systemic guarantees
using reflective methods, rather than appealing to external resources
or meta-systems.

[1] Willard 2020: https://arxiv.org/abs/2006.01057

[2] Specifically, Willard identifies a trade-off between the
arithmetic primitives available in the language of a formal system,
and its deduction method. (1) and (2) can be viewed as a "conserved
quantity"; the corresponding conservation law requires that more
expressive languages be paired with less efficient deduction
methods. The maximally expressive family of SJAS languages includes
successor and addition as total functions, but not multiplication,
which must be defined in terms of a relation atop the division
function. In exchange, this family of languages must use the method of
analytical tableaux or, it is conjectured [3], similar proof
techniques like resolution.

[3] "For the sake of simplicity, the previous sections had focused on
the semantic tableau deductive apparatus. However, it is known [15]
that resolution shares numerous characteristics with
tableau. Therefore, it turns out that Theorems 4.4 and 4.5 do
generalize when resolution replaces semantic tableau." - Willard 2020,
p18.

[4] https://ncatlab.org/nlab/show/Lawvere's+fixed+point+theorem

[5] Yanofksy 2003: https://arxiv.org/abs/math/0305282

[6] Oberhoff 2019: https://arxiv.org/abs/1909.04569

[7] Adjacent to the technical goals of the above, it is of great
interest whether there is a convergence between the features of SJAS
and those of the self-interpreter for the strongly normalizing lambda
calculus devised by Brown and Palsberg [7], which similarly speaks of
a barrier overcome by excluding diagonalization.

[8] Brown, Palsberg 2016: http://compilers.cs.ucla.edu/popl16/ 

[9] Willard 2014: https://arxiv.org/abs/1307.0150
#+END_PROGRAMME
:END:

:20220614Z:
Misc draft notes

[20220608Z]
The great limitative theorems of mathematical logic, paradigmatically
Goedel's Incompleteness Theorems, Tarski's Undefinability of Truth,
and Turing's Undecidability of the Halting Problem, all share a common
theme: the trade-off between expressivity and certainty. A system,
capable of generating objects with certain properties, cannot also
provide the guarantee that it will limit itself to the generation of
objects with only those properties. 

It is entirely possible that this guarantee can be obtained by some
other, more powerful system, but the greater can only vouch for the
lesser, not for itself, to which the same limits apply.

Examining more closely, this commonality is not only thematic but
technical: the nature of the expressivity-certainty trade-off is
directly mappable to the barrier between the countable (aleph-null)
and uncountable (aleph-one) infinities, as described by
Cantor. Cantor's Diagonalization Argument _demonstrating_ this
stratification in the infinitary hierarchy is likewise at the core of
common proofs of the limitative theorems, though specialized to their
particular circumstances. Lawvere's Fixed Point Theorem abstracts and
generalizes Cantorian Diagonalization within the domain of Category
Theory. provides an

Goedel's Incompleteness Theorems are a byword for the limits of formal
systems.

Dan Willard, in his work on Self-Justifying Axiom Systems, has
characterized the nature of the Incompleteness effect, and charted the
precise circumstances under which a system becomes too expressive to
avail itself of its own capacity for certainty in its expressions. As
a result, he has devised logics that skate as close to the edge of the
Incompleteness Theorems as possible, logics which can prove to
themselves that they are consistent, without

The great limitative theorems of mathematical logic place strong
constraints on the epistemic capabilities of formal symbol systems. 

The great limitative theorems of mathematical logic, paradigmatically
G T and Tu, all have a common theme, of the trade-off between
expressivity and certainty. More than a theme, they also have a common
technical structure, and can be proven using specialized variants of
Cantor's Diagonalization Argument, used originally to demonstrate the
distinction between aleph-null and aleph-one. Lawvere's Fixed Point
Theorem formalizes this observation, by providing a category theoretic
framework for diagonalization arguments _in general_. G1, T, and Tu,
among others, have already been proven using LFPT techniques, but not
G2.

information processing systems.


It is entirely possible that this guarantee can be obtained by some
other system - for any logical theory, semantic model, or programming
language, a more powerful alternative could sidestep the limits of the
less powerful system. But, because the limitative theorems extend
upwards, these more powerful systems would themselves be
susceptible. Thus, the search for a guarantee is open-ended.

Examining the commonality more closely, For Goedel, completeness and
consistency, for Tarski truth, for Turing halting
:END:

